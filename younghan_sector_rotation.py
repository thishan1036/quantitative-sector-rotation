# -*- coding: utf-8 -*-
"""youngHan_sector_rotation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gPKYrxNR528n1iCqemVCPya-nLK64-n_

Capstone Project

Single Stage Relative Sector Rotation Model

Objective:

To develop and validate a quantitative model that ranks the 11 sector ETFs based on predicted relative performance. The final model will be used to construct a monthly rotating portfolio that aims to outperform benchmarks while managing regime risk.

Dataset range: 2010-01-01 to present
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pandas_ta
!pip install yfinance

import pandas as pd
import numpy as np
from datetime import datetime, timedelta


import matplotlib.pyplot as plt
import seaborn as sns

import yfinance as yf
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import ElasticNet

from scipy import stats
from scipy.stats import zscore

import warnings
warnings.filterwarnings('ignore')
np.NaN = np.nan # this solved issue with pandas_ta
import pandas_ta as ta
from pandas.tseries.offsets import BDay

# internal function
# checking data quality
def quick_data_check(df, data_name = 'dataset'):
  print(f"{data_name}")
  print(f"Shape of {data_name}: {df.shape}")
  print(f"Data range: {df.index.min()} to {df.index.max()}")
  print(f"Missing values: {df.isnull().sum().sum()}")
  print(f"Columns: {df.columns.tolist()}")
  return df.head(3), df.tail(3)

# creating rolling features
def create_rolling_features(series, windows=[20,60,200]):
  features = {}
  for window in windows:
    features[f'ma_{window}'] = series.rolling(window=window).mean()
    features[f'std_{window}'] = series.rolling(window=window).std()
  return pd.DataFrame(features, index=series.index)

# creating sector features
def create_sector_features(price_series, windows=[20,60,200]):
  features = {}
  for window in windows:
    features[f'sector_ma_{window}'] = price_series.rolling(window=window).mean()
    features[f'sector_std_{window}'] = price_series.rolling(window=window).std()

  features['returns_1m'] = price_series.pct_change(20)
  features['returns_3m'] = price_series.pct_change(60)
  features['volatility_20d'] = price_series.pct_change().rolling(20).std()*np.sqrt(252)
  return pd.DataFrame(features, index=price_series.index)

"""## Part 1.1

### Data collection and preparation
"""

# data collection
start_date = '2010-01-01'
end_date = datetime.now().strftime('%Y-%m-%d')
print(f"Collecting data from {start_date} to {end_date}")

sector_etf = {
    'XLE': 'Energy',
    'XLF': 'Financials',
    'XLV': 'Health Care',
    'XLI': 'Industrials',
    'XLB': 'Materials',
    'XLRE': 'Real Estate',
    'XLK': 'Technology',
    'XLC': 'Communication Services',
    'XLY': 'Consumer Discretionary',
    'XLP': 'Consumer Staples',
    'XLU': 'Utilities'
}

benchmark = {
    'SPY': 'S&P 500',
    'SHY': '30-Year Treasury Bill'
}

macro_indicator = {
    '^TNX': '10-Year Treasury',
    'CL=F': 'WTI Crude Oil',
}

# sector ETFs
data = {}
for ticker, name in sector_etf.items():
  print(f"Downloading {ticker} ({name})")
  try:
    ticker_data = yf.download(ticker,
                              start = start_date,
                              end = end_date,
                              progress=False)
    data[ticker] = ticker_data
    print(f"{ticker}: {len(ticker_data)} rows downloaded")
  except Exception as e:
    print(f"Error downloading {ticker}: {e}")

# benchmarks
for ticker, name in benchmark.items():
  print(f"Downloading {ticker} ({name})")
  try:
    ticker_data = yf.download(ticker,
                              start = start_date,
                              end = end_date,
                              progress=False)
    data[ticker] = ticker_data
    print(f"{ticker}: {len(ticker_data)} rows downloaded")
  except Exception as e:
    print(f"Error downloading {ticker}: {e}")

# macro indicators
for ticker, name in macro_indicator.items():
  print(f"Downloading {ticker} ({name})")
  try:
    ticker_data = yf.download(ticker,
                              start = start_date,
                              end = end_date,
                              progress=False)
    data[ticker] = ticker_data
    print(f"{ticker}: {len(ticker_data)} rows downloaded")
  except Exception as e:
    print(f"Error downloading {ticker}: {e}")

print(f"Total number of datasets: {len(data)}")
print(f"List of datasets: {list(data.keys())}")

# extract adjusted close
adj_close_prices = {}
for ticker, ticker_data in data.items():
    adj_close_prices[ticker] = ticker_data[('Close', ticker)]

# create DataFrame
df_data = pd.DataFrame(adj_close_prices)

# validate dataset
quick_data_check(df_data, "Combined Price Data")

# missing data analysis
missing_val = df_data.isnull().sum().sort_values(ascending=False)
print(f"Missing values: \n{missing_val}\n")

for ticker in df_data.columns:
  print(f"{ticker}: {df_data[ticker].first_valid_index()}")

"""## Part 1.2

### Missing data (XLC, XLRE)

2 sector ETFs launched later than the other 9 ETFs and thus have missing data.
- XLRE: launched 2015-10-08
 - real estate ETF existed before 2015 will be used as proxy for XLRE
- XLC: launched 2018-06-19
 - top 10 holdings of XLC will be used as proxy for XLC before 2018
"""

# checking real estate ETFs availability
real_estate = ['VNQ','IYR','FREL','SCHH']
available_etf = []

for etf in real_estate:
  try:
    test_data = yf.download(etf,
                            start = '2010-01-01',
                            end = '2015-10-07',
                            progress=False)
    if not test_data.empty:
      available_etf.append(etf)
      print(f"{etf}: available from {test_data.index.date.min()}")
    else:
      print(f"{etf}: not available")
  except Exception as e:
    print(f"Error downloading {etf}: {e}")

# choose between VNQ and IYR by comparing correlation with XLRE
vnq = yf.download('VNQ',
                  start = '2015-10-07',
                  end = '2025-01-01',
                  progress=False)
iyr = yf.download('IYR',
                  start = '2015-10-07',
                  end = '2025-01-01',
                  progress=False)
xlre = yf.download('XLRE',
                  start = '2015-10-07',
                  end = '2025-01-01',
                  progress=False)

vnq_close = vnq['Close','VNQ']
iyr_close = iyr['Close','IYR']
xlre = df_data['XLRE'].loc['2015-10-07':'2025-01-01'].dropna()

vnq_corr = vnq_close.corr(xlre)
iyr_corr = iyr_close.corr(xlre)

print(f"Correlation between VNQ and XLRE: {vnq_corr:.3f}")
print(f"Correlation between IYR and XLRE: {iyr_corr:.3f}")

vnq_return = vnq_close.pct_change().dropna()
iyr_retutrn = iyr_close.pct_change().dropna()
xlre_return = xlre.pct_change().dropna()

vnq_ret_corr = vnq_return.corr(xlre_return)
iyr_ret_corr = iyr_retutrn.corr(xlre_return)

print(f"Correlation between VNQ returns and XLRE returns: {vnq_ret_corr:.3f}")
print(f"Correlation between IYR returns and XLRE returns: {iyr_ret_corr:.3f}")

print(f"Using IYR as proxy for XLRE")

# first day XLRE existed
stitch_date_xlre = '2015-10-08'

# proxy data up to (but not including) the stitch date
iyr_prices = yf.download("IYR", start='2010-01-01', end=stitch_date_xlre, progress=False)['Close'].squeeze()

# real data from the stitch date onwards
xlre_prices = yf.download("XLRE", start=stitch_date_xlre, end=end_date, progress=False)['Close'].squeeze()

# scaling factor
scaling_factor_xlre = xlre_prices.iloc[0] / iyr_prices.iloc[-1]

# scaling the entire proxy history
scaled_iyr_history = iyr_prices * scaling_factor_xlre

# result XLRE
continuous_xlre = pd.concat([
    scaled_iyr_history,
    xlre_prices
])

df_data['XLRE'] = continuous_xlre

quick_data_check(df_data, "Combined Price Data")

# XLRE stitching verification plot
plt.figure(figsize=(12,6))
plt.plot(scaled_iyr_history, label='Scaled IYR Proxy')
plt.plot(xlre_prices, label='Actual XLRE')
plt.axvline(pd.to_datetime(stitch_date_xlre), color='red', linestyle=':',
            label=f'Stitch Date: {stitch_date_xlre}')
plt.title('Verification of XLRE Price History Stitching')
plt.ylabel('Price')
plt.legend()
plt.show()

# checking telecommunication ETFs availability
tele_comm = ['FCOM','VOX','IYZ']
available_etf = []

for etf in tele_comm:
  try:
    test_data = yf.download(etf,
                            start = '2010-01-01',
                            end = '2018-06-19',
                            progress=False)
    if not test_data.empty:
      available_etf.append(etf)
      print(f"{etf}: available from {test_data.index.date.min()}")
    else:
      print(f"{etf}: not available")
  except Exception as e:
    print(f"Error downloading {etf}: {e}")

# choose between VOX and IYZ by comparing correlation with XLRE
vox = yf.download('VOX',
                  start = '2018-06-18',
                  end = '2025-01-01',
                  progress=False)
iyz = yf.download('IYZ',
                  start = '2018-06-18',
                  end = '2025-01-01',
                  progress=False)
xlc = yf.download('XLC',
                  start = '2018-06-18',
                  end = '2025-01-01',
                  progress=False)

vox_close = vox['Close','VOX']
iyz_close = iyz['Close','IYZ']
xlc = df_data['XLC'].loc['2018-06-18':'2025-01-01'].dropna()

vox_corr = vox_close.corr(xlc)
iyz_corr = iyz_close.corr(xlc)

print(f"Correlation between VOX and XLC: {vox_corr:.3f}")
print(f"Correlation between IYZ and XLC: {iyz_corr:.3f}")

vox_return = vox_close.pct_change().dropna()
iyz_return = iyz_close.pct_change().dropna()
xlc_return = xlc.pct_change().dropna()

vox_ret_corr = vox_return.corr(xlc_return)
iyz_ret_corr = iyz_return.corr(xlc_return)

print(f"Correlation between VOX returns and XLC returns: {vox_ret_corr:.3f}")
print(f"Correlation between IYZ returns and XLC returns: {iyz_ret_corr:.3f}")

print(f"Using VOX as proxy for XLC")

# scaling proxy etfs to mathc XLC price

# first day XLC existed
stitch_date_xlc = '2018-06-19'

# proxy data up to (but not including) the stitch date
vox_prices = yf.download("VOX", start='2010-01-01', end=stitch_date_xlc, progress=False)['Close'].squeeze()

# real data from the stitch date onwards
xlc_prices = yf.download("XLC", start=stitch_date_xlc, end=end_date, progress=False)['Close'].squeeze()

# scaling using the last day of the proxy and first day of the real asset
scaling_factor_xlc = xlc_prices.iloc[0] / vox_prices.iloc[-1]

# scaling the entire proxy history
scaled_vox_history = vox_prices * scaling_factor_xlc

# result XLC
continuous_xlc = pd.concat([
    scaled_vox_history,
    xlc_prices
])

df_data['XLC'] = continuous_xlc

quick_data_check(df_data, "Combined Price Data")

# XLC stitching verification plot
plt.figure(figsize=(12,6))
plt.plot(scaled_vox_history, label='Scaled VOX Proxy', linestyle='--')
plt.plot(xlc_prices, label='Actual XLC')
plt.axvline(pd.to_datetime(stitch_date_xlc), color='red', linestyle=':',
            label=f'Stitch Date: {stitch_date_xlc}')
plt.title('Verification of XLC Price History Stitching')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

# high yield credit spread (risk premium)
# difference between safer bonds and junk bonds
fred_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/BAMLH0A0HYM2.csv')
fred_data['observation_date'] = pd.to_datetime(fred_data['observation_date'])
fred_data.set_index('observation_date', inplace=True)
fred_data.rename(columns={'BAMLH0A0HYM2': 'HY_SPREAD'}, inplace=True)
df_data = df_data.merge(fred_data, left_index=True, right_index=True, how='left')

quick_data_check(df_data, "Combined Price Data")

df_data.to_csv('/content/drive/My Drive/Colab Notebooks/df_sector_rotation_data.csv')

"""## Part 2-1

### Feature set engineering
"""

# load and check duplicates
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/df_sector_rotation_data.csv')

print(f"Total rows: {len(df)}")
print(f"Unique dates: {df['Date'].nunique()}")

duplicate_dates = df[df['Date'].duplicated(keep=False)]
if not duplicate_dates.empty:
    print(duplicate_dates[['Date']].sort_values('Date'))

# prepare dataset
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)

sectors = ['XLE', 'XLF', 'XLV', 'XLI', 'XLB', 'XLRE',
           'XLK', 'XLC', 'XLY', 'XLP', 'XLU']

def create_feature_a(df, sectors):
  features_df = pd.DataFrame(index=df.index)

  for sector in sectors:
    prices = df[sector]
    returns = prices.pct_change()
    # return 63 days (3 month momentum)
    features_df[f'{sector}_return_63d'] = prices.pct_change(63)
    # return 252 days (1 year trend)
    features_df[f'{sector}_return_252d'] = prices.pct_change(252)
    # volatility 63 days (3 month volatility)
    features_df[f'{sector}_volatility_63d'] = returns.rolling(63).std() * np.sqrt(252)

    rolling_return = returns.rolling(63).mean()*252
    rolling_vol = returns.rolling(63).std()*np.sqrt(252)
    # sharpe 63 days (risk adjusted return)
    features_df[f'{sector}_sharpe_63d'] = rolling_return / rolling_vol

  # ranking by return 63 days (cross sectional relative strength)
  return_63d_cols = [col for col in features_df.columns if 'return_63d' in col]
  rank_df = features_df[return_63d_cols].rank(axis=1, method='min')
  for i, sector in enumerate(sectors):
    if f'{sector}_return_63d' in rank_df.columns:
      features_df[f'{sector}_rank_return_63d'] = rank_df[f'{sector}_return_63d']

  spy_200ma = df['SPY'].rolling(200).mean()
  # regime filter (if spy was above 200 day ma)
  features_df['spy_above_200ma'] = (df['SPY'] > spy_200ma).astype(int)
  # energy sector specifiec feature (oil price momentum)
  features_df['oil_price_momentum_63d'] = df['CL=F'].pct_change(63)
  # interest rate level
  features_df['treasury_10y_level'] = df['^TNX']
  # high yield credit spread (credit risk measure)
  features_df['hy_credit_spread'] = df['HY_SPREAD']

  return features_df

features_a = create_feature_a(df, sectors)

quick_data_check(features_a, "Feature Set A")

features_a.to_csv('/content/drive/My Drive/Colab Notebooks/features_set_a.csv')

features_b = features_a.copy()
features_b['treasury_10y_1m_change'] = df['^TNX'].pct_change(21)

quick_data_check(features_b, "Feature Set B")

features_b.to_csv('/content/drive/My Drive/Colab Notebooks/features_set_b.csv')

"""- feature set B will be mentioned in future work/steps

## Part 2-1-1

### Exploratory data analysis: Features
"""

# correlation heatmap

sector_returns = df[sectors].pct_change().dropna()
corr_mat = sector_returns.corr()

plt.figure(figsize=(10,8))
sns.heatmap(corr_mat, annot=True, cmap='Blues', fmt='.2f', linewidth=.5)
plt.title('Correlation Matrix of Daily Sector ETF Returns')
plt.show()

mean_corr = (corr_mat.sum()-1)/(len(sectors)-1)
#mean_corr.round(3).sort_values(ascending=True)
print(f"Mean correlation: \n{mean_corr.round(3).sort_values(ascending=True)}")

"""- all sectors are moderately to highly correlated with each other.
- most sectors move in the same direction.
- predictng relative sector performance is likely to be difficult.
- combining less correlated assets might reduce overall risk. if one sector goes down, less correlated sectors are less likely to follow.
- future strategy could attempt to rotate between defensive and cyclical sectors such as XLU, XLP, XLE, XLK, and XLI.
"""

# feature distribution
plt.figure(figsize=(8,3))
plt.hist(features_a['XLE_return_63d'].dropna(), bins=50,
         color='steelblue', edgecolor='white')
#sns.histplot(features_a['XLE_return_63d'].dropna(), kde=True, bins=50,color='steelblue')
plt.title('Distribution of XLE 63-Day Rolling Return (Momentum Feature)')
plt.xlabel('63-Day Return (%)')
plt.ylabel('Frequency')
plt.axvline(features_a['XLE_return_63d'].mean(), color='coral',
            linestyle='--', label='Mean')
plt.legend()
plt.show()

# feature distribution
plt.figure(figsize=(8,3))
plt.hist(features_a['XLI_return_63d'].dropna(), bins=50,
         color='steelblue', edgecolor='white')
#sns.histplot(features_a['XLE_return_63d'].dropna(), kde=True, bins=50,color='steelblue')
plt.title('Distribution of XLI 63-Day Rolling Return (Momentum Feature)')
plt.xlabel('63-Day Return (%)')
plt.ylabel('Frequency')
plt.axvline(features_a['XLI_return_63d'].mean(), color='coral',
            linestyle='--', label='Mean')
plt.legend()
plt.show()

# feature distribution
plt.figure(figsize=(10,6))
plt.hist(features_a['XLK_return_63d'].dropna(), bins=50,
         color='steelblue', edgecolor='white')
#sns.histplot(features_a['XLE_return_63d'].dropna(), kde=True, bins=50,color='steelblue')
plt.title('Distribution of XLK 63-Day Rolling Return (Momentum Feature)')
plt.xlabel('63-Day Return (%)')
plt.ylabel('Frequency')
plt.axvline(features_a['XLK_return_63d'].mean(), color='coral',
            linestyle='--', label='Mean')
plt.legend()
plt.show()

"""- XLI - highly correlated sector and returns are more in-line with the general market. results in a tighter and more predictable distribution of returns
- XLE, lower correlation due to its own unique and highly volatile factors like oil prices. this creates the potential for much wider and more extreme return swings.
- XLK,

## Part 2-2

### Target engineering
"""

# 21 day forward log returns for each sector
def create_forward_return(df, sectors):
  target_df = pd.DataFrame(index=df.index)
  for sector in sectors:
    if sector in df.columns:
      # 21 day forward log returns
      future_price = df[sector].shift(-21)
      current_price = df[sector]
      # log returns
      target_df[f'{sector}_return_21d_forward'] = np.log(future_price / current_price)

  return target_df

targets = create_forward_return(df, sectors)

quick_data_check(targets, "Target Set")

targets.to_csv('/content/drive/My Drive/Colab Notebooks/target_21d_forward.csv')

"""## Part 3-1

### Dataset preparation
"""

# merging features and targets
features_a['Date'] = pd.to_datetime(features_a.index)
features_a.set_index('Date', inplace=True)
features_b['Date'] = pd.to_datetime(features_b.index)
features_b.set_index('Date', inplace=True)
targets['Date'] = pd.to_datetime(targets.index)
targets.set_index('Date', inplace=True)

data_a = features_a.merge(targets, left_index=True, right_index=True, how='inner')
data_b = features_b.merge(targets, left_index=True, right_index=True, how='inner')

print(f"Features A shape: {data_a.shape}")
print(f"Features B shape: {data_b.shape}")
print(f"Data A shape: {data_a.shape}")
print(f"Data B shape: {data_b.shape}")

data_a.to_csv('/content/drive/My Drive/Colab Notebooks/modeling_data_a.csv')
data_b.to_csv('/content/drive/My Drive/Colab Notebooks/modeling_data_b.csv')

# checking infinite values
numeric_cols = data_a.select_dtypes(include=[np.number]).columns
inf_ct = []
for col in numeric_cols:
  if data_a[col].isin([np.inf, -np.inf]).any():
    inf_ct.append(col)
print(f"Infinite values: {inf_ct}")
if not inf_ct:
  print("No infinite values")

# checking outliers
for col in numeric_cols:
  q1 = data_a[col].quantile(.01)
  q99 = data_a[col].quantile(.99)
  outlier_low = (data_a[col] < q1).sum()
  outlier_high = (data_a[col] > q99).sum()
  total_outliers = outlier_low + outlier_high
  if total_outliers > 0:
    print(f"{col}: {total_outliers} outliers")
  else:
    print(f"{col}: no outliers")

q1 = data_a['XLE_return_63d'].quantile(.01)
q99 = data_a['XLE_return_63d'].quantile(.99)
print(f"Max loss: {q1}")
print(f"Max gain: {q99}")

outlier_mask = (data_a['XLE_return_63d'] < q1) | (data_a['XLE_return_63d'] > q99)
outliers_dates = data_a[outlier_mask].index
print(f"Outliers dates: \n{outliers_dates}")

"""- Outlier dates are valid ones.
- In March-June 2020, market experienced COVID crash and recovery
- In early 2021, market volatility went up with GameStop and meme stock boom
- In March-April 2022, Russia invaded Ukraine provoking inflation fears
"""

# missing values
missing_ct = data_a.isnull().sum().sort_values(ascending=False)
print(f"Missing values: \n{missing_ct}\n")
missing_pct = missing_ct / len(data_a) * 100
print(f"Missing values percentage: \n{missing_pct}\n")

total_missing = missing_ct.sum()
print(f"Total missing values: {total_missing}")
print(f"Total cells in dataset: {len(data_a) * len(data_a.columns)}")
print(f"Percentage of missing values: {total_missing / (len(data_a) * len(data_a.columns)) * 100}")

# forward filling missing values up to 5 days
# assuming the last known price persists until new information arrives
data_a_filled = data_a.fillna(method='ffill', limit=5)
print(f"Missing values before forward-fill: {total_missing}")
print(f"Missing values after forward-fill: {data_a_filled.isnull().sum().sum()}")

df_clean = data_a_filled.dropna()
print(f"Before cleaning: {data_a_filled.shape}")
print(f"After cleaning: {df_clean.shape}")
print(f"Data range: {df_clean.index.date.min()} to {df_clean.index.date.max()}")

df_clean.to_csv('/content/drive/My Drive/Colab Notebooks/modeling_data_a_clean.csv')

"""## Part 3-2

### Timeline setup

A strict T-2 timeline will be used to prevent look-ahead bias.
- Predictions are generated two days before month end and trades are executed on the last trading day of the month.
"""

month_ends = df_clean.resample('BM').last().index
t2_dates = month_ends - pd.tseries.offsets.BDay(2)
df_t2 = df_clean.loc[df_clean.index.isin(t2_dates)]

print(f"Original datast: {len(df_clean)} observations")
print(f"T-2 dataset: {len(df_t2)} observations")
print(f"Date range: {df_t2.index.date.min()} to {df_t2.index.date.max()}")

df_t2.to_csv('/content/drive/My Drive/Colab Notebooks/modeling_data_a_t2.csv')

"""## Part 3-3

### Walk forward validation

- The model will be tested using an expanding window, retrained monthly, with a minimum initial training period of 36 months.
- The model will be always tested on unseen data.
"""

# minimum training window = 36 months
min_train_month = 36
start_idx = min_train_month - 1

# walk forward validation
results = []
for i in range(start_idx, len(df_t2)):
  # expanding window from start to current by 1 month
  train_data = df_t2.iloc[:i+1]

  # next month
  if i + 1 < len(df_t2):
    test_data = df_t2.iloc[i+1:i+2]
    results.append({
        'train_end': train_data.index.date[-1],
        'test_date': test_data.index.date[0],
        'train_size': len(train_data)
    })

print(f"Total prediction: {len(results)}")
print(f"First prediction: {results[0]['test_date']}")
print(f"Last prediction: {results[-1]['test_date']}")

"""- 1st prediction: Train on 36 months (2011-2014), predict Feb 2014
- 2nd prediction: Train on 37 months (2011-2014), predict Mar 2014
- continues as long as data is available
"""

results_df = pd.DataFrame(results)
results_df.to_csv('/content/drive/My Drive/Colab Notebooks/walk_forward_df.csv')

"""## Part 4-1

### Random Forest Model
"""

# random forest
# separating target and features
feature_cols = [col for col in df_t2.columns if not col.endswith('_return_21d_forward')]
target_cols = [col for col in df_t2.columns if col.endswith('_return_21d_forward')]

rf_mod = RandomForestRegressor(n_estimators=100,
                               random_state=42,
                               max_depth=3)
scaler = StandardScaler()
rf_pred = []

for i, result in enumerate(results):
  # training data
  train_data = df_t2.iloc[:start_idx + i + 1]
  X_train = scaler.fit_transform(train_data[feature_cols])
  y_train = train_data[target_cols].values

  # testing data
  test_data = df_t2.iloc[start_idx + i + 1:start_idx + i + 2]
  X_test = scaler.transform(test_data[feature_cols])

  # predict
  rf_mod.fit(X_train, y_train)
  pred = rf_mod.predict(X_test)

  # results
  sector_dict = dict(zip([col.replace('_return_21d_forward','') for col in target_cols], pred[0]))
  rf_pred.append({'date': result['test_date'], **sector_dict})

print(f"Total predictions: {len(rf_pred)}")
print(f"First prediction: {rf_pred[0]['date']}")
print(f"Last prediction: {rf_pred[-1]['date']}")

# rankings
rf_pred_df = pd.DataFrame(rf_pred)
rf_pred_df.set_index('date', inplace=True)

rf_rankings = rf_pred_df.rank(axis=1, ascending=False)

print("Sample predictions:")
print(rf_pred_df.head(3))
print("\nSample rankings:")
print(rf_rankings.head(3))

# actual returns
actual_returns_df = df_t2[target_cols].copy()
actual_returns_df.columns = [col.replace('_return_21d_forward','') for col in target_cols]
actual_returns_df = actual_returns_df.loc[rf_pred_df.index]
actual_returns_rankings = actual_returns_df.rank(axis=1, ascending=False)

print("Actual returns:")
print(actual_returns_df.head(3))
print("\nActual rankings:")
print(actual_returns_rankings.head(3))

rank_corr = rf_rankings.corrwith(actual_returns_rankings, axis=1).mean()
print(f"Rank correlation: {rank_corr.mean()*100:.3f}") # no predictive power

# random forest accuracy
# % of time top 3 predicted match actual top 3
def calculate_top3_accuracy(pred_rankings, actual_rankings):
  top3_matches = 0
  total_predictions = len(pred_rankings)
  for i in range(len(rf_rankings)):
    # top 3 predicted sectors
    pred_top3 = set(rf_rankings.iloc[i].nsmallest(3).index)
    # top 3 actual sectors
    actual_top3 = set(actual_returns_rankings.iloc[i].nsmallest(3).index)

    overlap = len(set(pred_top3) & set(actual_top3))
    top3_matches += overlap / 3

  return top3_matches / total_predictions

rf_accuracy = calculate_top3_accuracy(rf_rankings, actual_returns_rankings)
print(f"Top 3 accuracy: {rf_accuracy:.1%}")

# direction accuracy for random forest
rf_direction_correct = 0
total_predictions = 0

for i in range(len(rf_pred_df)):
  for sector in rf_pred_df.columns:
    predicted_return = rf_pred_df.iloc[i][sector]
    actual_return = actual_returns_df.iloc[i][sector]

    if (predicted_return > 0 and actual_return > 0) or (predicted_return < 0 and actual_return < 0):
      rf_direction_correct += 1
    total_predictions += 1

rf_direction_accuracy = rf_direction_correct / total_predictions
print(f"Direction accuracy: {rf_direction_accuracy:.1%}")

"""## Part 4-2

### Elastic Net
"""

# elastic net
elnet_mod = ElasticNet(alpha=0.1,
                       l1_ratio=0.5,
                       random_state=42)
scaler = StandardScaler()
elnet_pred = []

for i, result in enumerate(results):
  # training data
  train_data = df_t2.iloc[:start_idx + i + 1]
  X_train = scaler.fit_transform(train_data[feature_cols])
  y_train = train_data[target_cols].values

  # testing data
  test_data = df_t2.iloc[start_idx + i + 1:start_idx + i + 2]
  X_test = scaler.transform(test_data[feature_cols])

  # predict
  elnet_mod.fit(X_train, y_train)
  pred = elnet_mod.predict(X_test)[0]

  # results
  sector_dict = dict(zip([col.replace('_return_21d_forward','') for col in target_cols], pred))
  elnet_pred.append({'date': result['test_date'], **sector_dict})

print(f"Elastic net total predictions: {len(elnet_pred)}")
print(f"First prediction: {elnet_pred[0]['date']}")
print(f"Last prediction: {elnet_pred[-1]['date']}")

# rankings
elnet_pred_df = pd.DataFrame(elnet_pred)
elnet_pred_df.set_index('date', inplace=True)
elnet_rankings = elnet_pred_df.rank(axis=1, ascending=False)

print("Sample predictions:")
print(elnet_pred_df.head(3))
print("\nActual returns:")
print(actual_returns_df.head(3))
print("\nSample rankings:")
print(elnet_rankings.head(3))
print("\nActual rankings:")
print(actual_returns_rankings.head(3))

# elastic net accuracy
total_correct = 0
total_possible = len(elnet_rankings) * 3

for i in range(len(elnet_rankings)):
  pred_top3 = set(elnet_rankings.iloc[i].nsmallest(3).index)
  actual_top3 = set(actual_returns_rankings.iloc[i].nsmallest(3).index)
  total_correct += len(pred_top3 & actual_top3)

elnet_accuracy = total_correct / total_possible
print(f"Top 3 accuracy: {elnet_accuracy:.1%}")

# direction accuracy for elastic net
elnet_direction_correct = 0
total_predictions = 0

for i in range(len(elnet_pred_df)):
  for sector in elnet_pred_df.columns:
    predicted_return = elnet_pred_df.iloc[i][sector]
    actual_return = actual_returns_df.iloc[i][sector]

    if (predicted_return > 0 and actual_return > 0) or (predicted_return < 0 and actual_return < 0):
      elnet_direction_correct += 1
    total_predictions += 1

elnet_direction_correct = elnet_direction_correct / total_predictions
print(f"Direction accuracy: {elnet_direction_correct:.1%}")

"""## Part 4-3

### Results
"""

# baseline
# random guessing top 3
import random
sim_accuracy = []
nrep = 1000

for i in range(nrep):
  total_mathes = 0
  for j in range(len(rf_rankings)):
    random_top3 = set(random.sample(rf_rankings.columns.tolist(), 3))
    actual_top3 = set(actual_returns_rankings.iloc[j].nsmallest(3).index)

    total_mathes += len(random_top3 & actual_top3)

  accuracy = total_mathes / (len(rf_rankings) *3)
  sim_accuracy.append(accuracy)

baseline = np.mean(sim_accuracy)
print(f"Baseline accuracy: {baseline:.1%}")

# random guessing direction
sim_direction_accuracy = []
nrep = 1000

for i in range(nrep):
  total_correct = 0
  total_predictions = 0
  for j in range(len(rf_rankings)):
    for sector in rf_rankings.columns:
      random_direction = random.choice([-1, 1])
      actual_return = actual_returns_df.iloc[j][sector]
      actual_direction = 1 if actual_return > 0 else -1

      if random_direction == actual_direction:
        total_correct += 1
      total_predictions += 1

  direction_accuracy = total_correct / total_predictions
  sim_direction_accuracy.append(direction_accuracy)

baseline_direction_accuracy = np.mean(sim_direction_accuracy)
print(f"Baseline direction accuracy: {baseline_direction_accuracy:.1%}")

# simple momentum baseline
# 63-day (3 month) return
momentum_baseline = [col for col in df_t2.columns if '_rank_return_63d' in col and not col.endswith('_forward')]

if momentum_baseline:
  momentum_data = df_t2[momentum_baseline].loc[rf_pred_df.index]
  momentum_rankings = momentum_data.rank(axis=1, ascending=False)
  momentum_accuracy = calculate_top3_accuracy(momentum_rankings, actual_returns_rankings)
  print(f"Momentum accuracy: {momentum_accuracy:.1%}")
else:
  print("No momentum data available")

# momentum direction baseline
momentum_direction_baseline = [col for col in df_t2.columns if '_return_63d' in col and 'rank' not in col]
momentum_data = df_t2[momentum_direction_baseline].loc[rf_pred_df.index]

momentum_direction_correct = 0
total_predictions = 0

for i in range(len(momentum_data)):
  for col in momentum_direction_baseline:
    sector = col.replace('_return_63d', '')

    momentum_return = momentum_data.iloc[i][col]
    predicted_direction = 1 if momentum_return > 0 else -1

    actual_return = actual_returns_df.iloc[i][sector]
    actual_direction = 1 if actual_return > 0 else -1

    if predicted_direction == actual_direction:
      momentum_direction_correct += 1
    total_predictions += 1

momentum_direction_accuracy = momentum_direction_correct / total_predictions
print(f"Momentum direction accuracy: {momentum_direction_accuracy:.1%}")

# result table
results = pd.DataFrame({
    'Model': ['Random Baseline', 'Momentum Baseline',
              'Random Forest', 'Elastic Net'],
    'Top 3 ETFs Accuracy': [baseline, momentum_accuracy,
                       rf_accuracy, elnet_accuracy],
    'Direction Accuracy': [baseline_direction_accuracy,
                           momentum_direction_accuracy,
                           rf_direction_accuracy,
                           elnet_direction_correct,]
})

results['Top 3 ETFs Accuracy'] = results[
    'Top 3 ETFs Accuracy'].apply(lambda x: f"{x*100:.1f}%")


results['Direction Accuracy'] = results[
    'Direction Accuracy'].apply(lambda x: f"{x*100:.1f}%")

print(results.to_string(index=False))

# top 3 accuracy comparison
models = ['Random\nBaseline', 'Momentum\nBaseline', 'Random\nForest', 'Elastic\nNet']
top3_scores = [baseline, momentum_accuracy, rf_accuracy, elnet_accuracy]

plt.figure(figsize=(8, 5))
bars = plt.bar(models, top3_scores, width=0.6,
               color=['lightgray', 'lightblue', 'skyblue', 'steelblue'])
plt.axhline(y=baseline, color='black', linestyle=':',
            label=f'Random Baseline: {baseline:.1%}')
plt.ylabel('Top-3 Accuracy (%)')
plt.title('Ranking Top-3 ETFs Accuracy Comparison')
plt.show()

# direction accuracy comparison
direction_scores = [baseline_direction_accuracy, momentum_direction_accuracy,
                    rf_direction_accuracy, elnet_direction_correct]
plt.figure(figsize=(8,5))
bars = plt.bar(models, direction_scores, width=0.6,
               color=['lightgray', 'lightblue', 'skyblue', 'steelblue'])
plt.axhline(y=baseline_direction_accuracy, color='black', linestyle=':',
            label=f'Random Baseline: {baseline_direction_accuracy:.1%}')
plt.ylabel('Direction Accuracy (%)')
plt.title('Direction Accuracy Comparison')
plt.legend()
plt.show()

"""Result analysis & feature importance analysis

model	Top 3 accuracy
Random forest	29.8
Elastic net	27.1
Random baseline	27.2
Momentum baseline	29.8
model	Direction accuracy
Random forest	55
Elastic net	60.1
Random baseline	50
Momentum baseline	51.6
Result analysis
Both models failed to predict which sector ETFs will be top 3
Elastic net performed worst while random forest was on par with momentum baseline.
This shows momentum based features might have high importance among the features.

Both models showed better direction accuracy than momentum baseline.
However, 60.1% direction accuracy by elastic net seems suspiciously high. Investigation starts here.
Need to check feature importance


Feature importance analysis

Random forest
1. Macro Features Are Genuinely Weak (More Likely)
* Market reality: Individual sector momentum/fundamentals matter more than broad macro
* Feature design issue: Maybe macro features need transformation (e.g., changes vs levels)
* Time horizon mismatch: 21-day predictions might be too short for macro effects
2. Model Implementation Issues
* Feature scaling: Macro features might have different scales
* Interaction effects: Macro might only matter in combination with other features
* Non-linear relationships: Random Forest might miss complex macro patterns

The Main Story: It's an Energy Model, Not a Sector Model

The most immediate and powerful story is that your Random Forest model isn't functioning as a balanced, all-sector rotation model. Instead, it has effectively become a specialized momentum model that is overwhelmingly driven by the Energy sector (XLE).
Here are the key plot points for that story:
* Extreme Concentration: The top 4 features alone account for over 45% of the model's predictive power (0.160 + 0.136 + 0.091 + 0.067). Three of these four features belong to a single sector: XLE. The model is basing a huge portion of its decisions for all 11 sectors on the behavior of just one or two of them.
* Momentum is King: The most important features are all related to momentum (_return_63d, _return_252d) or risk-adjusted momentum (_sharpe_63d). This tells you the model has learned a simple primary rule: "the best predictor of a sector's future performance is its own recent performance." This makes your planned comparison against a simple momentum baseline absolutely critical.
* Macro Signals are Ignored: As you noted, the model has decided that the broad macro features you provided are far less important than the specific momentum of individual sectors. The model is essentially saying, "I don't need to know about interest rates or credit risk if I already know how the Energy sector has been performing."
* The model's 55% direction accuracy heavily relies on Energy sector patterns - it's not learning general market dynamics, just Energy-specific signals

Why is the Energy Sector so Dominant?

You are right to ask this question. The model has latched onto XLE for a few distinct reasons:
1. High Volatility and Strong Trends: The Energy sector is notoriously volatile and cyclical. It experiences massive booms and busts tied to the price of oil. For a tree-based model like Random Forest, this is great! It creates very strong, clear signals (big up-trends, big down-trends) that are easy to use for splitting data and making rules. Other, less volatile sectors provide weaker, noisier signals that the model finds less useful.
2. Unique Driver: Unlike tech or healthcare, Energy's fate is tied directly to a single, globally traded commodity. This makes its behavior fundamentally different from the other 10 sectors, which are more closely tied to the general business cycle. This uniqueness makes it a powerful, and apparently dominant, predictive feature.
So, to answer your question: Yes, your model believes Energy is the most "reactive" and informative sector, not necessarily against the whole market, but in a way that its own momentum provides the clearest signal for future returns.

## Part 5-1

### Random forest analysis (feature importance)
"""

# feature importance
from sklearn.inspection import permutation_importance
# random forest
rf_feature_importance = rf_mod.feature_importances_
rf_importance_df = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': rf_feature_importance
}).sort_values('Importance', ascending=False)

print(f"Random Forest top 30 Feature Importance:")
print(rf_importance_df.head(30))

# random forest feature importance plot
top5_features = rf_importance_df.head(5)
macro_features = rf_importance_df[rf_importance_df['Feature'].isin([
    'hy_credit_spread','oil_price_momentum_63d',
    'treasury_10y_level','spy_above_200ma'
])]

plot_data = pd.concat([top5_features, macro_features]).drop_duplicates()

plt.figure(figsize=(8, 5))
plt.barh(plot_data['Feature'], plot_data['Importance']*100,
         color=['steelblue']*5 + ['coral']*len(macro_features))
plt.xlabel('Feature Importance (%)')
plt.title('Random Forest Feature Importance: Top Features vs Macro Variables')
plt.gca().invert_yaxis()
plt.show()

"""- extreme concentration on single sector, XLE, accouts more than 30% of the model's predictive power
- model is basing a huge portion of its decision on 11 sectors based on behavior of one or two sectors (XLE, XLF)
- macro features show weaker importance compared to momentum features
- regime macro feature based on SPY is completely ignored
"""

# check SPY above 200MA counts
spy_regime = df_t2['spy_above_200ma'].value_counts()
print(spy_regime)
print(f"Percentage above 200MA: {spy_regime[1]/len(df_t2):.1%}")

# SPY feature barplot
plt.figure(figsize=(5, 5))
plt.bar([1,0], spy_regime.values, color=['steelblue','lightgray'], width=.5,
        label=[f'Bull: 82.2%', f'Bear: 17.8%'])
plt.xlabel('SPY above 200-Day MA')
plt.ylabel('Number of Months')
plt.title('Market Regime Distribution (Bull vs Bear)')
plt.xticks([1,0], ['Above 200MA\n(Bull)','Below 200MA\n(Bear)'])
plt.legend()
plt.show()

"""- 82.2% of the time SPY was above 200MA
- Only 30 months of bear market
- 18% bear market period is not enough to create meaningful patters for random forest
- Model learned everything goes up which made regime feature useless because it was dominated by bull market
- This leads to suspicion that direction accuracy coming from bull market persistence, and not from modeling

## Part 5-2

### Elastic net analysis (feature importance)
"""

# elastic net
elnet_feature_importance = np.abs(elnet_mod.coef_).mean(axis=0)
elnet_importance_df = pd.DataFrame({
    'Feature': feature_cols,
    'Coefficient': elnet_feature_importance
}).sort_values('Coefficient', ascending=False)

print(f"Elastic Net top 10 Feature Importance:")
print(elnet_importance_df.head(10))

# elastic net feature importance plot
# blank plot
top5_features = elnet_importance_df.head(5)
macro_features = elnet_importance_df[elnet_importance_df['Feature'].isin([
    'hy_credit_spread','oil_price_momentum_63d',
    'treasury_10y_level','spy_above_200ma'
])]

plot_data = pd.concat([top5_features, macro_features]).drop_duplicates()

plt.figure(figsize=(10, 6))
plt.barh(plot_data['Feature'], plot_data['Coefficient']*100,
         color=['steelblue']*5 + ['coral']*len(macro_features))
plt.xlabel('Feature Importance (%)')
plt.title('Random Forest Feature Importance: Top Features vs Macro Variables')
plt.gca().invert_yaxis()
plt.show()

"""- elastic net ignored all features
- its 60% direction accuracy came from learning 82% bull market dominance

"""

# elastic net positive rate
pos_pred = (elnet_pred_df > 0).sum().sum()
tot_pred = elnet_pred_df.size
pred_pos_rate = pos_pred / tot_pred
print(f"Elastic net positive predictions: {pred_pos_rate:.1%}")

# positive rate by sector
print("\nPositive by sector:")
print((elnet_pred_df > 0).mean())

"""- elastic net ignored all features and predicted 97.1% positive based on its interept
- elastic net learned that, on average, sectors go up, so it predicted always up
- this explains why random forest feature importance relied heavily on XLE. XLE was the only sector with meaningful variation.

## Part 6-1

### Backtesting - random forest
"""

# backtesting random forest model performance
def simple_backtest(rankings, actual_returns, top_n=3):
  # initialize an empty list to store the monthly returns
  portfolio_return = []

  # loop through each month in test period
  for i in range(len(rankings)):
    # find the names of the top 3 sectors for this month
    top_sectors = rankings.iloc[i].nsmallest(top_n).index
    # equal weight portfolio return for this month
    month_return = actual_returns.iloc[i][top_sectors].mean()
    portfolio_return.append(month_return)

  return pd.Series(portfolio_return, index=rankings.index)

# random forest portfolio backtest
rf_portfolio_returns = simple_backtest(rf_rankings, actual_returns_df, top_n=3)

rf_annual_return = rf_portfolio_returns.mean() * 12
rf_volatility = rf_portfolio_returns.std() * np.sqrt(12)
rf_sharpe_ratio = rf_annual_return / rf_volatility

print(f"Random Forest Portfolio Performance:")
print(f"Annualized Return: {rf_annual_return:.1%}%")
print(f"Annualized Volatility: {rf_volatility:.1%}%")
print(f"Sharpe Ratio: {rf_sharpe_ratio:.2f}")

# baseline result (investing SPY only without sector rotation)
# SPY data setup
spy_df = pd.DataFrame({
    'SPY': df_data['SPY'],
    'spy_return_21d_forward': np.log(df_data['SPY'].shift(-21) / df_data['SPY'])
}, index=df_data.index)

# merge spy_df into df_clean
df_clean_with_spy = df_clean.merge(spy_df, left_index=True, right_index=True,
                                   how='left')

# create T-2 dataset with new dataset
month_ends = df_clean_with_spy.resample('BM').last().index
t2_dates = month_ends - pd.tseries.offsets.BDay(2)
df_t2_with_spy = df_clean_with_spy.loc[df_clean_with_spy.index.isin(t2_dates)]

# SPY only backtest
spy_returns_backtest = df_t2_with_spy['spy_return_21d_forward'].loc[rf_portfolio_returns.index]


spy_annual_return = spy_returns_backtest.mean() * 12
spy_volatility = spy_returns_backtest.std() * np.sqrt(12)
spy_sharpe_ratio = spy_annual_return / spy_volatility
print(f"SPY data shape: {spy_returns_backtest.shape}")
print(f"First date: {spy_returns_backtest.index.date[0]}")
print(f"Last date: {spy_returns_backtest.index.date[-1]}")

# SPY performance metrics
print("SPY Benchmark Performance:")
print(f"Annualized Return: {spy_annual_return:.1%}")
print(f"Annualized Volatility: {spy_volatility:.1%}")
print(f"Sharpe Ratio: {spy_sharpe_ratio:.2f}")

print("\nComparison:")
print("Random Forest vs SPY:")
print(f"RF Return: {rf_annual_return:.1%} vs SPY Return: {spy_annual_return:.1%}")

# RF vs SPY buy and hold ($1 initial investment)
plt.figure(figsize=(12,6))

rf_cumulative = (1 + rf_portfolio_returns).cumprod()
spy_cumulative = (1 + spy_returns_backtest).cumprod()

plt.plot(rf_cumulative, label='Random Forest Strategy', color='steelblue',
         linewidth=2)
plt.plot(spy_cumulative, label='SPY Banchmark', color='darkgray',
         linewidth=2)
plt.title('Cumulative Returns: RF Sector Rotation vs SPY')
plt.ylabel('Portfolio Value ($1 Initial Investment)')
plt.legend()
plt.show()

rf_metrics_data = pd.DataFrame({
    'Random Forest': [rf_annual_return*100, rf_volatility*100, rf_sharpe_ratio],
    'SPY Benchmark': [spy_annual_return*100, spy_volatility*100, spy_sharpe_ratio]
}, index=['Annual Return (%)','Volatility (%)','Sharpe Ratio'])

fig, ax = plt.subplots(1,3, figsize=(9,4))

for i, metric in enumerate(rf_metrics_data.index):
  ax[i].bar(['RF Strategy', 'SPY Benchmark'], rf_metrics_data.iloc[i],
            color=['steelblue', 'darkgray'], width=.8)
  ax[i].set_title(metric)
  ax[i].set_ylabel(metric)

plt.tight_layout()
plt.show()

"""- random forest sector rotation strategy shows slightly higher performance than buy and hold SPY strategy.
- however, the difference is too small to trade off transaction cost and expenses. cost of buying and selling sector ETFs every month would far exceed cost of just buying and holding SPY.

## Part 6-2

### Backtesting - elastic net
"""

# elastic net portfolio backtest
en_portfolio_returns = simple_backtest(elnet_rankings, actual_returns_df, top_n=3)

en_annual_return = en_portfolio_returns.mean() * 12
en_volatility = en_portfolio_returns.std() * np.sqrt(12)
en_sharpe_ratio = en_annual_return / en_volatility

print(f"Elastic Net Portfolio Performance:")
print(f"Annualized Return: {en_annual_return:.1%}")
print(f"Annualized Volatility: {en_volatility:.1%}")
print(f"Sharpe Ratio: {en_sharpe_ratio:.2f}")

# SPY performance metrics
print("SPY Benchmark Performance:")
print(f"Annualized Return: {spy_annual_return:.1%}")
print(f"Annualized Volatility: {spy_volatility:.1%}")
print(f"Sharpe Ratio: {spy_sharpe_ratio:.2f}")

print("\nComparison:")
print("Random Forest vs SPY:")
print(f"RF Return: {en_annual_return:.1%} vs SPY Return: {spy_annual_return:.1%}")

# EN vs SPY buy and hold ($1 initial investment)
plt.figure(figsize=(12,6))

en_cumulative = (1 + en_portfolio_returns).cumprod()
spy_cumulative = (1 + spy_returns_backtest).cumprod()

plt.plot(en_cumulative, label='Elastic Net Strategy', color='steelblue',
         linewidth=2)
plt.plot(spy_cumulative, label='SPY Banchmark', color='darkgray',
         linewidth=2)
plt.title('Cumulative Returns: EN Sector Rotation vs SPY')
plt.ylabel('Portfolio Value ($1 Initial Investment)')
plt.legend()
plt.show()

en_metrics_data = pd.DataFrame({
    'Elastic Net': [en_annual_return*100, en_volatility*100, en_sharpe_ratio],
    'SPY Benchmark': [spy_annual_return*100, spy_volatility*100, spy_sharpe_ratio]
}, index=['Annual Return (%)','Volatility (%)','Sharpe Ratio'])

fig, ax = plt.subplots(1,3, figsize=(9,4))

for i, metric in enumerate(en_metrics_data.index):
  ax[i].bar(['EN Strategy', 'SPY Benchmark'], en_metrics_data.iloc[i],
            color=['steelblue', 'darkgray'], width=.8)
  ax[i].set_title(metric)
  ax[i].set_ylabel(metric)

plt.tight_layout()
plt.show()

"""- Elastic net sector rotation underperformed SPY buy and hold strategy."""

# EN direction backtesting
def direction_backtest(predictions_df, spy_returns):
    portfolio_returns = []
    for i in range(len(predictions_df)):
      avg_prediction = predictions_df.iloc[i].mean()
      if avg_prediction > 0:
        month_return = spy_returns.iloc[i]
      else:
        month_return = 0.0

      portfolio_returns.append(month_return)

    return pd.Series(portfolio_returns, index=predictions_df.index)

en_direction_portfolio = direction_backtest(elnet_pred_df, spy_returns_backtest)

en_direction_annual = en_direction_portfolio.mean() * 12
en_direction_volatility = en_direction_portfolio.std() * np.sqrt(12)
en_direction_sharpe = en_direction_annual / en_direction_volatility

print(f"Elastic Net Direction Strategy Performance:")
print(f"Annualized Return: {en_direction_annual:.1%}")
print(f"Annualized Volatility: {en_direction_volatility:.1%}")
print(f"Sharpe Ratio: {en_direction_sharpe:.2f}")

# compare all EN strategies against SPY buy and hold
direction_comparison = pd.DataFrame({
    'EN Direction Strategy': [en_direction_annual*100, en_direction_volatility*100, en_direction_sharpe],
    'SPY Buy-and-Hold': [spy_annual_return*100, spy_volatility*100, spy_sharpe_ratio],
    'EN Sector Rotation': [en_annual_return*100, en_volatility*100, en_sharpe_ratio]
}, index=['Annual Return (%)', 'Volatility (%)', 'Sharpe Ratio'])

print("Strategy Comparison:")
print(direction_comparison)

# EN strategies vs SPY buy and hold ($1 initial investment)
plt.figure(figsize=(12, 6))

en_direction_cumulative = (1 + en_direction_portfolio).cumprod()
spy_cumulative = (1 + spy_returns_backtest).cumprod()
en_sector_cumulative = (1 + en_portfolio_returns).cumprod()

plt.plot(en_direction_cumulative.index, en_direction_cumulative.values,
         label='EN Direction Strategy', linewidth=2, color='steelblue')
plt.plot(spy_cumulative.index, spy_cumulative.values,
         label='SPY Buy-and-Hold', linewidth=2, color='darkorange', linestyle='--')
plt.plot(en_sector_cumulative.index, en_sector_cumulative.values,
         label='EN Sector Rotation', linewidth=2, color='darkgray')

plt.title('Elastic Net Strategies vs SPY Benchmark')
plt.ylabel('Portfolio Value ($1 Initial Investment)')
plt.legend()
plt.show()

"""- elastic net's superior direction accuracy turned out tobe meaningless - it just learned to always predict 'up' during a bull market period. the resulting market timing strategy was identical to passive buy and hold SPY stragety.
- model learned environment bias and no predictive signals
- both en strategies failed - one underperformed, the other just identical to the benchmark whild paying additional expense cost.
- random forest direction strategy likely similar since both models learned bull market bias.

## Conclusion

### Lessons learned

- engineer more sophisticated macro features
 - momentum features work better than macro features for short term prediction
 - cross-sectional ranking features didn't help as expected
 - time horizon mismatch; macro trends need longer time horizon
- might have discovered energy sector predictable model
 - results revealed that XLE momentum and volatility patterns are learnable
 - energy sector behaves differently from other sectors
- market regime matters
 - 82% bull market is a crucial and decisive factor
 - proves importance of testing models across different market conditions
 - reveals how models can learn environmental bias

### Limitation of current approach

-
- need more and rich data

### Future research/work
"""

